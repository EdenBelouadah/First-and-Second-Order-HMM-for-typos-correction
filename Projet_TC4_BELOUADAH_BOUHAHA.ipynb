{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First and second order HMM for typos correction \n",
    "## Belouadah Eden & Bouhaha Mariem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce projet est de mettre en pratique le modèle HMM avec l'algorithme de Viterbi pour la correction des fautes de frappes. \n",
    "Les observations dans la chaine de Markov sont les lettres tapées par l'utilisateur et les états (cachés) sont les vraies lettres correspondantes.  Le but étant de corriger la séquence des lettres, c'est à dire d'essayer de trouver la séquence des états cachés.\n",
    "\n",
    "La base de données utilisée pour ce projet est générée à partir du document \"Nabomber's Manifesto\". Deux variantes sont mises à notre disposition. Etant donné le text d'origine, la première variante (\"train10\" et \"test10\") est obtenue en donnant une probabilité de 0.1 pour que la lettre soit aléatoirement générée à la place de la lettre originale; et la deuxième variante (\"train20\" et \"test20\") est obtenue en donnant une probabilité de 0.2 pour que la lettre soit aléatoirement générée à la place de la lettre originale. La deuxième variante n'est qu'une version plus difficile de la première."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM du premier ordre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le HMM du premier ordre s'appuie sur l'hypothèse que l'état courant ne dépend que de l'état précédent. Cette hypothèse simplificatrice permet d'avoir un modèle simple, rapide et efficace.\n",
    "\n",
    "On commence par importer les modules nécessaires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #Pour la gestion des tableaux et des matrices\n",
    "import pickle #Pour la lecture de la base de données\n",
    "from nltk.probability import FreqDist #Pour les dictionnaires de comptes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement de la base de données: Pour changer la base de données, il suffit de choisir (\"train10\" et \"test10\") ou bien (\"train20\" et \"test20\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensApprentissage = pickle.load(open('train10.pkl', 'rb'))\n",
    "ensTest = pickle.load(open('test10.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation des hyper-paramètres du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inconnu = 0  #Encodage des mots inconnus (peu fréquents)\n",
    "facteurLissage = 0.0001 #Pour éviter d'avoir un dénominateur nul. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptage du nombre d'observations (lettres tapées) possibles, le nombre d'états (lettres corrects) possibles, le nombre de transitions possibles entre les états,  le nombre de fois où chaque état apparait au début d'une séquence (mot) et le nombre de pairs d'états possibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cptTapees = FreqDist() #Nombre d'observations possibles\n",
    "cptCorrectes = FreqDist() #Nombre d'états possibles\n",
    "cptBigramCorrectes = FreqDist() #Nombre de pairs d'états\n",
    "cptTransitions = FreqDist() #Nombre de transitions entre états\n",
    "cptDebut = FreqDist() #Nombre de fois où les états apparaissent au début d'un mot\n",
    "for mot in ensApprentissage: #Pour chaque mot de l'ensemble d'apprentissage\n",
    "    for i in range(len(mot)): #Pour chaque lettre du mot (la lettre contient l'observation+état caché)\n",
    "        tapee_correcte = mot[i] #Récupérer le couple (observation,état)\n",
    "        tapee = tapee_correcte[0] #Observation\n",
    "        correcte = tapee_correcte[1] #Etat\n",
    "        #Compteur des lettres tapées (les observations)\n",
    "        cptTapees[tapee] += 1\n",
    "        #Compteur des lettres correctes (les états)\n",
    "        cptCorrectes[correcte] += 1\n",
    "        #Compteur des bigrammes des états\n",
    "        cptBigramCorrectes[tapee_correcte] += 1\n",
    "        if i==0: #Compteur des états initiaux\n",
    "            cptDebut[correcte] += 1       \n",
    "        else: #Compteur des transitions entre les états\n",
    "            cptTransitions[(mot[i-1][1], correcte)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérer la liste des observations possibles, la liste des états possibles et leurs tailles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations=list(cptTapees.keys())#Les observations sont les clés du dictionnaire des comptes  des lettres tapées\n",
    "etats=list(cptCorrectes.keys())#Les états sont les clés du dictionnaire des compte des lettres correctes\n",
    "N=len(etats)#Nombre d'états\n",
    "M=len(observations)#Nombre d'observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer des dictionnaires qui permettent de passer d'une lettre (observation/état) à son encodage en entier correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictTapees = {}\n",
    "dictCorrectes = {}\n",
    "#Coder l'état par sa position\n",
    "for i in range(len(etats)):\n",
    "    dictCorrectes[etats[i]] = i\n",
    "for i in range( len(observations)):\n",
    "    dictTapees[observations[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer A (matrice des probabilités de transitions), B (matrice de probabilités d'observations) et Pi (vecteur des probabilités initiales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = np.zeros((N,N), float) #matrice des probabilités de transitions\n",
    "B = np.zeros((M,N), float) #matrice de probabilités d'observations\n",
    "Pi = np.zeros((N,), float) #Vecteur des probabilités initiales des états\n",
    "#######################################################\n",
    "#Vecteur Pi\n",
    "for correcte in cptDebut: #Pour chaque état\n",
    "    i = dictCorrectes[correcte] #Récupérer sa position\n",
    "    Pi[i] = cptDebut[correcte] #Récupérer son nombre d'apparition\n",
    "Pi = Pi / sum(Pi) #Calculer la moyenne\n",
    "########################################################\n",
    "#Matrice A:\n",
    "for tapee_correcte in cptTransitions: \n",
    "    i = dictCorrectes[tapee_correcte[1]] #Récupérer la position de l'état\n",
    "    j = dictCorrectes[tapee_correcte[0]] #Récupérer la position de l'observation\n",
    "    A[i, j] = cptTransitions[tapee_correcte] #Récupérer leur nombre d'apparition ensemble\n",
    "A = A / A.sum(axis=0).reshape(1, N) #Calculer la moyenne\n",
    "########################################################\n",
    "#Matrice B:\n",
    "for tapee_correcte in cptBigramCorrectes:\n",
    "    tapee = tapee_correcte[0] #Récupérer l'observation\n",
    "    correcte = tapee_correcte[1] #Récuoérer l'état\n",
    "    if tapee in dictTapees: #On fait cette vérification parce qu'il peut  y avoir des caractères non alphabétics introduits par erreur de frappe\n",
    "        i = dictTapees[tapee] #Récupérer la position de l'observation\n",
    "    j = dictCorrectes[correcte] #Pas de vérification d'appartenance car on est sûr que c'est vérifié\n",
    "    B[i, j] = cptBigramCorrectes[tapee_correcte] #Récupérer le nombre d'apparition\n",
    "B = B + facteurLissage #Ajouter le facteur de lissage\n",
    "B = B / B.sum(axis=0).reshape(1, N) #Calculer la moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui permet de transformer chaque lettre d'un mot en entrée en un entier correspondant unique, pour pouvoir indexer les matrices de probabilités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CoderMot(mot, dictTapees, dictCorrectes, inconnu):\n",
    "    codeTapees = []\n",
    "    codeCorrectes = []\n",
    "    for tapee_correcte in mot: #Pour chaque couple (état, observation)\n",
    "        tapee = tapee_correcte[0]\n",
    "        correcte = tapee_correcte[1]\n",
    "        if tapee in dictTapees:\n",
    "            codeTapees.append(dictTapees[tapee]) #Récupérer le code correspondant à l'observation si elle existe dans le dictionnaire\n",
    "        else:\n",
    "            codeTapees.append(inconnu) #Mettre le code du caractère inconnu (0)\n",
    "        codeCorrectes.append(dictCorrectes[correcte]) #Récupérer le code correspondant à l'état\n",
    "    return codeTapees, codeCorrectes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai d'un classifieur stupide qui ne fait rien, donc il suffit de calculer le taux des lettres erronées parmis les lettres de la séquence observée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision d'un classifieur stupide = 89.82% \n"
     ]
    }
   ],
   "source": [
    "cptBienCorrigees = 0\n",
    "somme = 0\n",
    "for mot in ensTest: #Pour chaque mot de l'ensemble de test\n",
    "    motTape, motCorrect = CoderMot(mot, dictTapees, dictCorrectes, inconnu) #Récupérer le mot en tant que codes\n",
    "    cptBienCorrigees += sum(np.array(motCorrect)==np.array(motTape)) #Accumuler le nombre de lettres bien corrigés\n",
    "    somme += len(motTape) #Accumuler le nombre total de lettres \n",
    "print (\"Précision d'un classifieur stupide = {:.2f}% \".format(float(cptBienCorrigees)/somme*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui implémente l'algorithme Viterbi, qui permet, à partir d'une séquence de lettres en entrées, de produire la séquences de de lettres cachées correspondantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Viterbi1(motTape, N, A, B, Pi):\n",
    "    T = len(motTape)\n",
    "    psi = np.zeros((T, N), int) #Matrice qui va contenir le poids de chaque etat pour chaque lettre du mot tape\n",
    "    delta_courant = np.zeros(N, float) #On ne garde que deux vecteurs (précédent et courant) pour parcourir psi\n",
    "    delta_precedent = np.zeros(N, float)\n",
    "    etats_caches = [] #C'est le resultat qu'on veut construire\n",
    "    delta_courant = B[motTape[0]] * Pi #Initialisation pour la première itération\n",
    "    for t in range(1, T): #Pour le reste des itérations\n",
    "        lettre_courante = motTape[t] #Récupérer la lettre courante du mot tape\n",
    "        for j in range(N): #Pour chaque état possible\n",
    "            interm=delta_courant * A[j, :]\n",
    "            psi[t, j] = interm.argmax() #Récupérer le meilleur état caché\n",
    "            delta_precedent[j] = interm[interm.argmax()] * B[lettre_courante, j]\n",
    "        #Permutation:\n",
    "        z=delta_precedent\n",
    "        delta_precedent=delta_courant\n",
    "        delta_courant=z\n",
    "    #Choisir le meilleur état caché pour la dernière observation\n",
    "    etats_caches.append(delta_courant.argmax())\n",
    "    for i in range(psi.shape[0]-1, 0, -1): #Récupérer les meilleurs états des observations précédentes\n",
    "        etats_caches.append(psi[i, etats_caches[-1]])\n",
    "    etats_caches.reverse() #Inverser la liste et la renvoyer\n",
    "    return etats_caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai d'un classifieur HMM du premier ordre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision de l'HMM du premier ordre=93.20%\n",
      "Nombre de lettres bien corrigées = 6822\n",
      "Nombre de lettres mal corrigées = 498\n"
     ]
    }
   ],
   "source": [
    "cptBienCorrigees = 0\n",
    "somme = 0\n",
    "for mot in ensTest: #Pour chaque mot de l'ensemble de test\n",
    "    motTape, motCorrect=CoderMot(mot, dictTapees, dictCorrectes, inconnu) #Récupérer le mot en tant que codes\n",
    "    motCorrige = Viterbi1(motTape, N, A, B, Pi) #Récupérer la séquence d'états cachés retournés par l'algorithme de Viterbi\n",
    "    cptBienCorrigees += sum(np.array(motCorrect)==np.array(motCorrige)) #Accumuler le nombre de lettres bien corrigés\n",
    "    somme += len(motTape) #Accumuler le nombre total de lettres \n",
    "print(\"Précision de l'HMM du premier ordre={:.2f}%\".format(float(cptBienCorrigees)/somme*100))\n",
    "print (\"Nombre de lettres bien corrigées = \" + str(cptBienCorrigees))\n",
    "print (\"Nombre de lettres mal corrigées = \" + str(somme-cptBienCorrigees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nous remarquons que le classifieur Markovien du premier ordre est meilleur que le classifieur stupide. En effet ce dernier a donné une précision de 89.82% sur l'ensemble de test alors que HMM1 a donné 93.20% Mais cela n'est pas vraiment suffisant, car l'écrat entre les deux n'est pas grand. Ceci dit qu'on préfère ne rien faire et obtenir un score de 89% que d'entraîner tout un modèle \"couteûx\" en temps et en calcul pour avoir un score de 93%. Cela reste bien sur dans le cadre de ce problème seulement et n'est pas une généralisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HMM du deuxième ordre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dans ce modèle, nous ne tenons pas compte seulement de la lettre précédente comme historique mais plutôt des deux lettres précédentes afin de rendre le modèle un peu plus complexe mais plus efficace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La base de données étant déja chargée et certains comptes sont déjà calculés, nous nous contentons d'ajouter les comptes des trigrammes de lettres et les compte de transitions trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cptTrigramCorrectes=FreqDist()\n",
    "cptTransitionsTrigram=FreqDist()\n",
    "for mot in ensApprentissage: #Pour chaque mot de l'ensemble d'apprentissage\n",
    "    for i in range(len(mot)):\n",
    "        tapee_correcte=mot[i]\n",
    "        tapee = tapee_correcte[0] #Récupérer l'observation\n",
    "        correcte = tapee_correcte[1] #Récupérer l'état\n",
    "        #Compteur des trigrammes\n",
    "        if i > 0 and i<len(mot)-1:\n",
    "            cptTrigramCorrectes[(mot[i-1][1],correcte)]+=1\n",
    "        #Compteur des transitions entre les trigrammes\n",
    "        if i > 1:\n",
    "            cptTransitionsTrigram[(mot[i-2][1],mot[i-1][1],correcte)]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération la liste des trigrammes possibles des états et sa taille:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrammes=list(cptTrigramCorrectes.keys())#Les trigrammes sont les clés du dictionnaire des comptes des trigrammes d'états\n",
    "L = len(trigrammes) #Longueur de la liste des trigrammes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer le dictionnaire d'encodage des trigrammes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictTrigram = {}\n",
    "for i in range(L): #Encoder le trigramme par sa position\n",
    "    dictTrigram[trigrammes[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les matrices de probabilités restent les mêmes sauf qu'il faut rajouter une matrice pour les transition trigrammes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = np.zeros((N, L), float) #Matrice des transitions des trigrammes\n",
    "for trigramme in cptTransitionsTrigram:\n",
    "    i=dictCorrectes[trigramme[2]] #Récupérer la position de l'état\n",
    "    j=dictTrigram[(trigramme[0],trigramme[1])] #Récupérer la position de l'observation1 et l'observation2\n",
    "    C[i,j]=cptTransitionsTrigram[trigramme] #Calculer le nombre d'appartition\n",
    "C=C/C.sum(axis=0).reshape(1,L) #Faire la moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les comptes et les probabilités nécessaires sont calculées, l'algorithme de Viterbi devient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Viterbi2(motTape, N, L, A, B, C, Pi, dictTrigram, observations, inconnu):\n",
    "    T = len(motTape)\n",
    "    delta_courantt = np.zeros( N, float )\n",
    "    delta_precedent = np.zeros( N, float )\n",
    "    psi = np.zeros( (T, N), int )      \n",
    "    #Initialisation pour la première itération\n",
    "    delta_courant = B[motTape[0]] * Pi \n",
    "    etats_caches = [] #Liste des états cachés qu'on veut construire\n",
    "    for t in range(1, T): #Pour chaque lettre du mot tapé à partir de la deuxième\n",
    "        if (t==1): #S'il s'agit de la deuxième lettre\n",
    "            lettre_courante = motTape[t] #Récupérer la lettre\n",
    "            for j in range(N): #Pour chaque état possible\n",
    "                interm=delta_courant * A[j,:]\n",
    "                psi[t, j] = interm.argmax()       \n",
    "                delta_precedent[j] = interm[interm.argmax()] * B[lettre_courante, j] \n",
    "            #Permutation:\n",
    "            z=delta_courant\n",
    "            delta_courant=delta_precedent\n",
    "            delta_precedent=z\n",
    "        else: #S'il s'agit de la troisième lettre ou plus\n",
    "            lettre_courante = motTape[t] #Récupérer la lettre\n",
    "            for j in range(N):\n",
    "                for x in range(N):\n",
    "                    y=psi[t-1,x]\n",
    "                    bigramObservations=(observations[y],observations[x])\n",
    "                    if bigramObservations in dictTrigram:\n",
    "                        interm[x]=delta_courant[x]*C[j,dictTrigram[bigramObservations]]\n",
    "                    else:\n",
    "                        interm[x]=inconnu\n",
    "                psi[t, j] = interm.argmax()  #Récupérer le meilleur état caché     \n",
    "                delta_precedent[j] = interm[interm.argmax()] * B[lettre_courante, j] \n",
    "            #Permutation:\n",
    "            z=delta_courant\n",
    "            delta_courant=delta_precedent\n",
    "            delta_precedent=z\n",
    "    #Choisir le meilleur état caché pour la dernière observation\n",
    "    etats_caches.append(delta_courant.argmax())\n",
    "    for i in range(psi.shape[0]-1, 0, -1): #Récupérer les meilleurs états des observations précédentes\n",
    "        etats_caches.append(psi[i, etats_caches[-1]])              \n",
    "    etats_caches.reverse()#Inverser la liste et la renvoyer\n",
    "    return etats_caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tout est prêt pour calculer la précision de HMM2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision de l'HMM du deuxième ordre=94.34%\n",
      "Nombre de lettres bien corrigées = 6906\n",
      "Nombre de lettres mal corrigées = 414\n"
     ]
    }
   ],
   "source": [
    "cptBienCorrigees = 0\n",
    "somme = 0\n",
    "for mot in ensTest: #Pour chaque mot de l'ensemble de test\n",
    "    motTape, motCorrect=CoderMot(mot, dictTapees, dictCorrectes, inconnu) #Récupérer le mot en tant que codes\n",
    "    motCorrige =Viterbi2(motTape, N, L, A, B, C, Pi, dictTrigram, observations, inconnu) #Récupérer la séquence d'états cachés retournés par l'algorithme de Viterbi\n",
    "    cptBienCorrigees += sum(np.array(motCorrect)==np.array(motCorrige)) #Accumuler le nombre de lettres bien corrigés\n",
    "    somme += len(motTape) #Accumuler le nombre total de lettres \n",
    "print(\"Précision de l'HMM du deuxième ordre={:.2f}%\".format(float(cptBienCorrigees)/somme*100))\n",
    "print (\"Nombre de lettres bien corrigées = \" + str(cptBienCorrigees))\n",
    "print (\"Nombre de lettres mal corrigées = \" + str(somme-cptBienCorrigees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons qu'effectivement, le modèle HMM du deuxième ordre (94.34%) est plus efficace que HMM du premier ordre (93.18%). Le nombre de lettres bien corrigées qui était égal à 6822 a augmenté jusqu'à 6906, et le nombre de lettres mal corrigées qui était égal à 498 est diminué jusqu'à 414.\n",
    "\n",
    "Nous avons gagné un peu dans la précision et nous avons perdu un peu en termes de temps de calcul, mais pour ce problème, le temps de calcul n'est pas important, en augmantant la taille de l'historique, le modèle devient de plus en plus complexe et couteûx mais bien sûr plus efficace. Il reste qu'à faire le compromis entre ces deux critères selon le type de problèmes qu'on veut traiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **HMM1** | | **HMM2** |\n",
    "------------ | -------------\n",
    "| **Bien corrigées** | **Précision** | **Bien corrigées** | **Précision**\n",
    "**BDD_10** | 6822/7320 | 93.20% | 6906/7320 | 94.34%\n",
    "**BDD_20** | 14499/16691 | 86.87%| 14949/16691 | 89.56%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critics and evolution: This model is somehow limited. For instance, it only handles substitution errors. Could you describe a way to extend this model to also handle noisy insertion of characters ?  Same question for deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nous remarquons que ce modèle marche pour les erreurs de substitution seulement, ceci dit que la longueur de la séquence d'observations et la longueur de la séquence d'états est la même. Le problème d'ajout et de suppression de caractères est une limite pour le modèle HMM de base. Cependant, plusieurs méthodes ont vu le jour afin de modifier HMM pour permettre ce genre de traitements. Parmi les travaux récents nous citons [1].\n",
    "Les auteurs ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised training: Propose and discuss some ideas to do unsupervised training for this task. For this question you should provide details on : what kind of data, which parameters will be involved, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1/ L'idée la plus simple est d'utiliser un dictionnaire de mots (français ou anglais...) implémenté sous forme de dictionnaire (structure de données) où les clés sont les mots du langage et les valeurs sont vides. Pour chaque mot tapé par l'utilisateur, nous testons si le mot existe comme clé dans le dictionnaire. Si c'est le cas, aucune correction ne sera faite. Sinon, on peut calculer la similarité entre le mot tapé et les clés du dictionnaires (par distance de Levenshtein par exemple ou par une méthode de recherche d'information comme TF-IDF). La liste des mots similaires ordonnés par ordre décroissant de similarité sont proposés pour la correction du mot.\n",
    "\n",
    "L'inconvénient majeur de cette approche est la nécessité d'avoir un dictionnaire le plus riche possible de la langue qu'on traite. De plus, le calcul de similarité avec un dictionnaire aussi grand entraîenra un temps de calcul important. Finalement, la décision peut être délicate en cas où certains mots du dictionnaire ont la même valeur de similarité avec le mot tapé.\n",
    "\n",
    "2/ Puisque la distance entre les mots peut être mesurée par la méthode de Levenshtein , cela peut nous faire penser à utiliser le clustering (par k-means ou k-medoids...) sur le dictionnaire des mots. Après que les regroupements deviennent stables et leurs centroids/medoids sont calculés, nous calculons la distance du le mot tapé par l'utilisateur avec les centroids/medoids des clusters, on l'attribut au cluster le plus proche. Autrement, les mots du cluster choisi seront proposés pour la correction du mot tapé.\n",
    "\n",
    "L'inconvénient de cette méthode, mise à part la nécessité d'un dictionnaire et le temps de calcul important, c'est que sa qualité dépend de la qualité du clustering, qui dépend elle aussi de l'initialisation aléatoire des centroids/médoids des clusters et du nombre de clusters choisi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet était pour nous une occasion pour mettre en pratique l'algorithme de Viterbi avec le modèle HMM. La correction des fautes de frappe est un domaine qui peut s'étendre au domaine médical comme par exemple pour la comparaison des séquence d'ADN. L'algorithme de Viterbi n'est qu'une variante de plusieurs algorithmes qui se basent sur des techniques variées telles que le dictionnaire ...etc, chaque méthode a ses avantages et ses inconvénients et chaque méthode est plus appropriées que les autres pour certains types d'applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Noroozi, Vahid, \"Probabilistic insertion, deletion and substitution error correction using Markov inference in next generation sequencing reads\" (2016).Graduate Theses and Dissertations. 15097. http://lib.dr.iastate.edu/etd/15097"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
